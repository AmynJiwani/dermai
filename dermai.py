# -*- coding: utf-8 -*-
"""DermAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1exyIDtcNbcCHBuyOcr8Bxte-giEAxQ62
"""

# Written by: Amyn Jiwani
# This script allows for the importing, parsing, training and visualization of a dataset of skin conditions
# Note: This script is still a work in progress

# Step 1: Install Required Libraries
!pip install tensorflow numpy matplotlib

# Step 2: Import Libraries
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import os
import glob

# Step 3: Define Functions to Load and Parse the Dataset
def parse_tfrecord_fn(example):
    keys_to_features = {
        'image/encoded': tf.io.FixedLenFeature([], tf.string),
        'image/filename': tf.io.FixedLenFeature([], tf.string),
        'image/format': tf.io.FixedLenFeature([], tf.string),
        'image/height': tf.io.FixedLenFeature([], tf.int64),
        'image/width': tf.io.FixedLenFeature([], tf.int64),
        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),
        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),
        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),
        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),
        'image/object/class/label': tf.io.VarLenFeature(tf.int64),
    }
    parsed_features = tf.io.parse_single_example(example, keys_to_features)
    image = tf.io.decode_jpeg(parsed_features['image/encoded'], channels=3)
    image = tf.image.resize(image, [224, 224])
    label = tf.sparse.to_dense(parsed_features['image/object/class/label'])
    return image, label

# Step 4: Load TFRecord dataset
tfrecord_dir = r"C:\Users\Owner\Downloads\DermAI Unedited DB.v3i.folder"  # Used raw string
print("Directory path:", tfrecord_dir)

# List files in the directory to ensure they are present
tfrecord_files = glob.glob(os.path.join(tfrecord_dir, "*.tfrecord"))
print("TFRecord files found:", tfrecord_files)

if not tfrecord_files:
    print("No TFRecord files found in the specified directory. Please check the path and ensure files are present.")
else:
    # Step 5: Read the TFRecord files
    raw_dataset = tf.data.TFRecordDataset(tfrecord_files)

    # Parse the dataset
    parsed_dataset = raw_dataset.map(parse_tfrecord_fn)

    # Step 6: Calculate Dataset Length
    def get_dataset_length(dataset):
        return sum(1 for _ in dataset)

    dataset_length = get_dataset_length(parsed_dataset)
    print(f"Dataset length: {dataset_length}")

    if dataset_length == 0:
        print("The dataset is empty. Please check the TFRecord files for data.")
    else:
        # Shuffle and batch the dataset
        BATCH_SIZE = 32
        parsed_dataset = parsed_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)

        # Step 7: Split the Dataset
        train_size = int(0.8 * dataset_length)
        val_size = dataset_length - train_size
        train_dataset = parsed_dataset.take(train_size)
        val_dataset = parsed_dataset.skip(train_size).take(val_size)

        # Step 8: Create and Train the Model
        model = tf.keras.models.Sequential([
            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
            tf.keras.layers.MaxPooling2D((2, 2)),
            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
            tf.keras.layers.MaxPooling2D((2, 2)),
            tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
            tf.keras.layers.MaxPooling2D((2, 2)),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(512, activation='relu'),
            tf.keras.layers.Dense(1, activation='sigmoid')  # Adjust the output layer according to your classification problem
        ])

        model.compile(optimizer='adam',
                      loss='binary_crossentropy',  # or categorical_crossentropy for multi-class classification
                      metrics=['accuracy'])

        model.fit(train_dataset, epochs=10, validation_data=val_dataset)

        # Save the model
        model.save("skin_lesion_detector.h5")

        # Load the trained model (optional, if not already in memory)
        model = tf.keras.models.load_model("skin_lesion_detector.h5")

        # Step 9: Visualize the Dataset
        def display_sample(raw_dataset, num_samples=5):
            parsed_dataset = raw_dataset.map(parse_tfrecord_fn)
            plt.figure(figsize=(10, 10))
            for i, (image, label) in enumerate(parsed_dataset.take(num_samples)):
                plt.subplot(1, num_samples, i + 1)
                plt.imshow(image.numpy().astype("uint8"))
                plt.title(f"Label: {label.numpy()[0]}")
                plt.axis('off')
            plt.show()

        # Display a few samples from the dataset
        display_sample(raw_dataset)

        # Step 10: Visualize Model Predictions
        def visualize_predictions(model, raw_dataset, num_samples=5):
            parsed_dataset = raw_dataset.map(parse_tfrecord_fn)
            plt.figure(figsize=(10, 10))
            for i, (image, label) in enumerate(parsed_dataset.take(num_samples)):
                prediction = model.predict(tf.expand_dims(image, axis=0))
                predicted_label = np.argmax(prediction, axis=1)

                plt.subplot(1, num_samples, i + 1)
                plt.imshow(image.numpy().astype("uint8"))
                plt.title(f"Pred: {predicted_label[0]}, True: {label.numpy()[0]}")
                plt.axis('off')
            plt.show()

        # Visualize model predictions
        visualize_predictions(model, raw_dataset)